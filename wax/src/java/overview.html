    <html>
<head>
   <title>NutchWAX</title>
</head>
<body>
NutchWAX (<i>"<a href="http://nutch.org">Nutch</a> + Web
Archive eXtensions"</i>) searches web archive collections.
The Web Archive eXtensions (WAX) include adaptation of the Nutch fetcher
step to go against web archives rather than crawl the open net --
adaptation currently does 
<a href="http://www.archive.org">Internet Archive</a>
<a href="http://www.archive.org/web/researcher/ArcFileFormat.php">ARC
files</a> only -- and plugins to add extra fields to the index that
return an Archive Records' location in the repository, its collection
name, etc.

<h2><a name="toc"/>Table of Contents</h2>
<ul>
<li><a href="#getting_started">Getting Started</a>
<ul>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#standalone">Standalone Mode</a></li>
<li><a href="#pseudo">Pseudo-distributed Mode</a></li>
</ul>
</li>
<li><a href="#next">Where to go next</a></li>
<li><a href="#configuration">NutchWAX Configuration</a></li>
<li><a href="#src">Building NutchWAX from source</a></li>
</ul>
    

<h2><a name="getting_started">Getting Started</a></h2>
<p>NutchWAX runs on hadoop.  The general pattern is you ask the hadoop
platform to run one of the set of bundled mapreduce jobs from the NutchWAX
jar.   Whether the job runs on one machine or many or whether it uses
local disk or the hadoop distributed file system is a matter of hadoop
configuration.</p>  

<h3><a name="requirements">Requirements</a></h3>
<ol>
   <li><b>Linux</b>: NutchWAX may run on systems other than linux but linux is
the only OS we've tested it on.</li>  
  <li><b>Java</b>: NutchWAX requires a 1.5.x or above JDK.
  </li>
  <li><b>Servlet Container</b>: The NutchWAX search war has been tested
  working in version 5.0.28 and 5.5.x of
  <a href="http://tomcat.apache.org/">tomcat</a>.
  </li>
  <li><b><a name="hadoop">Hadoop</a></b>:
<a href="http://lucene.apache.org/hadoop">hadoop</a> is
the platform we use to run indexing jobs atop.  Hadoop is an open source
implementation of <a href="http://labs.google.com/papers/mapreduce.html">Google
mapreduce</a> and <a href="http://labs.google.com/papers/gfs.html">Google
GFS</a>.  NutchWAX 0.10.0 requires Hadoop 0.9.2.  It will not work with later
versions.  Hadoop has its own set of requirements.  See
<i>Requirements</i> about midways down on the
<a href="http://lucene.apache.org/hadoop/docs/api/overview-summary.html">Hadoop API</a>
page.  Hadoop binaries are available for download off the 
<a href="http://www.apache.org/dyn/closer.cgi/lucene/hadoop/">apache site</a>.
The NutchWAX README.txt lists patches made to Hadoop indexing on Internet Archive
hardware.
  </li>
</ol>

<h3><a name="standalone" />Running NutchWAX in non-distributed,
<i>Standalone</i> mode</h3>
<p>In this section we'll index a few ARCs and then setup the NutchWAX war file
to run queries against the produced index.  We will run NutchWAX
on a hadoop platform set to run non-distributed, all on a single box,
with all hadoop functions performed in a single process using the local
filesystem: i.e.  <i>Standalone operation</i>.</p>

<p>Before you can begin indexing, you must first get your hadoop platform up
and running.  See 
<a href="http://lucene.apache.org/hadoop/docs/api/overview-summary.html">hadoop
API</a> <i>Getting Started</i> section for how to install and configure your
hadoop platform.  Review <i>Standalone operation</i>.  Run the <i>Grep</i>
example.  Ensure your setup is error free by inspecting emissions on STDOUT
and the contents of the hadoop logs subdirectory Let your Hadoop install be
located at <code>${HADOOP_HOME}</code>.
</p>

<p>Download the NutchWAX binary from sourceforge (or from under <i>Build
Artifacts</i> on the archive's
<a href="http://builds.archive.org:8080/cruisecontrol/buildresults/HEAD-archive-access">continuous build server</a>).
Undo the NutchWAX <code>tar.gz</code>
bundle.  Let the unbundled NutchWAX binary be at <code>${NUTCHWAX_HOME}</code>.
</p>

<p>The NutchWAX binary has within it, a NutchWAX jar and a NutchWAX war file.
The jar is used at indexing time.  The war is for searching run in a servlet
container to field search queries.  The jar contains all code and supporting
libraries to run six distinct mapreduce jobs: 
<ul>
<li>Import of ARCs
</li>
<li>Update of the database of all URLs
</li>
<li>Invertion of the database of link information in readyness for,
</li>
<li>Indexing
</li>
<li>Deduplication
</li>
<li>and finally Merge of all indices.
</li>
</ul>
The above are permutations on Nutch operations only in our case we've
amended the nutch fetch to instead import ARCs, and to the other steps
we've added plugins to add in extra NutchWAX facility such as extra
fields in the index.
</p>
<h4>Indexing</h4>
<p>First, lets set up our environment variables for NutchWAX and HADOOP and
list out the NutchWAX usage  (You run the NutchWAX jar in the
same manner used above running Grep out of the 
<code>hadoop-*-examples.jar</code>).  After listing the general usage, we list
the particular usage for the <code>all</code> NutchWAX command.  

<pre>
  %  export HADOOP_HOME=/home/stack/tmp/nwtesting/hadoop-nightly/
  %  export NUTCHWAX_HOME=/home/stack/tmp/nwtesting/nutchwax
  %  ${HADOOP_HOME}/bin/hadoop jar ${NUTCHWAX_HOME}/nutchwax.jar
        Usage: hadoop jar nutchwax.jar &lt;job&gt; [args]
        Launch NutchWAX job(s) on a hadoop platform.
        Type 'hadoop jar nutchwax.jar help &lt;job&gt;' for help on a specific job.
        Jobs (usually) must be run in the order listed below.
        Available jobs:
         import  Import ARCs.
         update  Update dbs with recent imports.
         invert  Invert links.
         index   Index segments.
         dedup   Deduplicate by URL or content MD5.
         merge   Merge segment indices into one.
         all     Runs all above jobs in order.
         class   Run the passed class's main.
  %  ${HADOOP_HOME}/bin/hadoop jar ${NUTCHWAX_HOME}/nutchwax.jar help all
        Usage: hadoop jar nutchwax.jar all &lt;input&gt; &lt;output&gt; &lt;collection&gt;
        Arguments:
         input       Directory of files listing ARC URLs to import.
         output      Directory write indexing product to.
         collection  Collection name. Added to each resource.</pre>
</p>
<p>Review the output.  Now, lets index a couple of ARCs.  Of note, inputs for
mapreduce tasks are always directories not files.  To specify the ARCs to index,
we pass a path to a directory that has a file listing the ARCs to index.</p> 
<p>Lets assume we want to index the ARCs
<code>1.arc.gz</code>, <code>2.arc.gz</code>, and <code>3.arc.gz</code>.
First we make a file that lists the full-path to the ARCs-to-index. 
Here's how the file content might look:
<pre>
/tmp/1.arc.gz
/tmp/2.arc.gz
/tmp/3.arc.gz
</pre>
...assuming the ARCs are in <code>/tmp</code>.  Let this file be
<code>/tmp/inputs/arcs.txt</code>.</p>
<p><i>Note, you could also point to the ARCs using
URLs as in <code>http://localhost/~stack/1.arc.gz</code>, etc. assuming the
ARC was in stack's published web directory on localhost.</i>.  NutchWAX
includes URL Handlers for an rsync scheme that does a <code>Runtime.exec</code>
of <code>rsync</code> and a pseudo-S3 scheme for pulling ARCs out of 
Amazons' S3 simple storage service.</p> 
<p>Now lets run all of the indexing steps in one go by passing the <i>all</i>
directive to NutchWAX.  Have the indexing steps store their output to
a directory <code>/tmp/outputs</code> and let the collection name for
this test indexing be <i>test</i>.
<pre>
  % ${HADOOP_HOME}/bin/hadoop jar ${NUTCHWAX_HOME}/nutchwax.jar all /tmp/inputs /tmp/outputs test
</pre>
</p>
<p>It will run for a while stepping through each of the indexing stages (You
might want to redirect the output to a log file).
When done, you can inspect the <code>/tmp/outputs</code> directory.  It should
look the following:
<pre>
 % ls /tmp/outputs
	crawldb index indexes linkdb segments
</pre>
</p>

<h4><a name="searching" />Searching</h4>
<p>Deploy the NutchWAX war file under a Servlet Container such as
tomcat by copying it to the container's <code>webapps</code> folder.
The Servlet Container
will (usually -- unless configured not to) expand the deployed war file.
After expansion, go into the expanded war directory -- it will be named for the war
file absent the <code>.war</code> suffix -- and edit the <code>hadoop-site.xml</code>
file in the <code>WEB-INF/classes</code> subdirectory.
Add the following properties
between the <code>configuration</code> elements: 
<pre>
&lt;property>
  &lt;name>searcher.dir&lt;/name>
  &lt;value>${FULLPATH-OUTPUT-DIR-ON-LOCAL-FILESYSTEM}&lt;/value>
  &lt;description>
  Used at search time by the nutchwax webapp.

  Path to root of crawl.  This directory is searched (in
  order) for either the file search-servers.txt, containing a list of
  distributed search servers, or the directory "index" containing
  merged indexes, or the directory "segments" containing segment
  indexes.

  Set to an absolute path.  The alternative is having to start the
  container -- e.g. tomcat -- so its current working directory contains
  a subdirectory named 'searcher.dir'.
  &lt;/description>
&lt;/property>

&lt;property>
  &lt;name>wax.host&lt;/name>
  &lt;value>${COLLECTIONS_HOST}&lt;/value>
  &lt;description>
  Used at search time by the nutchwax webapp.
 
  The name of the server hosting collections.
  Used by the webapp conjuring URLs that point to page renderor
  (e.g. wayback).

  URLs are conjured in this fashion:

    ${wax.host}/COLLECTION/DATE/URL

  To override the COLLECTION obtained from the search result,
  add a path to wax.host: e.g. localhost:8080/web.
  &lt;/description>
&lt;/property>

</pre>

...replacing <code>${FULLPATH-OUTPUT-DIR-ON-LOCAL-FILESYSTEM}</code> with the
full path to the output directory, e.g. <code>/tmp/outputs</code>, and
<code>${COLLECTIONS_HOST}</code> to the name of the host running an application
that can render found web pages such as the 
<a href="http://archive-access.sourceforge.net/projects/wayback/">open-source
wayback</a> or 
<a href="http://archive-access.sourceforge.net/projects/wera/">WERA</a>.  If
you do not want to edit your war <i>in-situ</i> under webapps, set the
configuration before deploy by unpacking the war, making your changes, and
then repacking the war file.  See the
<a href="http://tomcat.apache.org/tomcat-5.0-doc/deployer-howto.html">tomcat-deployer</a>
application for a tool to help do this that works with the tomcat container
(recommended!).</p> 

<p>Now, browse to
where your container is running -- usually on port 8080 -- and add to the
path the name of the webapp: E.g. if the deployed webapp was named
<code>nutchwax</code>, then browse to
<code>http://container-host:8080/nutchwax</code>.  You should see the
NutchWAX query box.  Try some queries with terms you know to be present in the
indexed ARCs. If using tomcat, see your <code>${TOMCAT_HOME}/logs</code>, particularly
<code>catalina.out</code>, if
the webapp does not deploy successfully or if there are no search results
returned.
</p>

<h3><a name="pseudo">Pseudo-distributed Mode</a></h3>
<p>Now lets try running your indexing job in distributed mode using the
Hadoop Distributed File System (HDFS or DFS) rather than local store. 
The most basic <i>Distributed Operation</i> mode is that which is described
in the <i>Pseudo-distributed Configuration</i> section of the hadoop
documentation, where all daemons -- the controlling job daemon,
<i>JobTracker</i>, the controlling file system daemon, <i>NameNode</i>, and the
job slave, <i>TaskTracker</i> and data slave, <i>DataTracker</i> -- all run on
a single machine.  Ensure <code>ssh</code> is setup according to the hadoop
instructions (i.e. passwordless ssh login works),
that you've configured your <code>hadoop-site.xml</code> with locations of
mapreduce, and DFS head nodes, and that you've bootstrapped your DFS using the
<code>-format</code> command.  
</p>
<p>To access DFS, use the DFS client:
<code>${HADOOP_HOME}/bin/hadoop dfs</code>. 
Try it.  You will get a list of all
commands for the DFS file system (They generally work like their UNIX file
system counterparts). 
</p>
<p>Now, upload our file of ARCs to index, <code>/tmp/inputs/arcs.txt</code> to
DFS into the DFS directory named <code>inputs</code>.
<pre>
  % ${HADOOP_HOME}/bin/hadoop dfs -mkdir inputs
  % ${HADOOP_HOME}/bin/hadoop dfs -put /tmp/inputs/arcs.txt inputs
  % ${HADOOP_HOME}/bin/hadoop dfs -ls inputs
</pre>
</p>
<p>As we did in standalone mode above, run all of the indexing steps in one go
by passing the 'all' directive to NutchWAX but this time we'll be working 
against the distributed filesystem.  The 'inputs' and 'outputs' directories
specified above refer to locations up on the distributed filesystem (because
above we configured hadoop to use DFS instead of the local filesystem) .
 <pre>
  %{HADOOP_HOME}/bin/hadoop jar ${NUTCHWAX_HOME}/nutchwax.jar all inputs outputs test
</pre>
<i>Be aware that NutchWAX runs sub-optimally when
all daemons are hosted on a single computer but this mode is good for
familiarizing yourself with distributed operation without the headache of
multiple machines</i>.
</p>
<p>When done, you can inspect the DFS outputs directory.  It should look
the following:
 <pre>
 % ${HADOOP_HOME}/bin/hadoop dfs -ls outputs
    060425 183147 parsing file:/home/stack/tmp/nwtesting/hadoop-nightly/conf/hadoop-default.xml
    060425 183147 parsing file:/home/stack/tmp/nwtesting/hadoop-nightly/conf/hadoop-site.xml
    060425 183147 No FS indicated, using default:localhost:9000
    060425 183147 Client connection to 127.0.0.1:9000: starting
    Found 5 items
    /user/stack/outputs/crawldb     &lt;dir>
    /user/stack/outputs/index       &lt;dir>
    /user/stack/outputs/indexes     &lt;dir>
    /user/stack/outputs/linkdb      &lt;dir>
    /user/stack/outputs/segments    &lt;dir> </pre>
</p>

<p>While its possible to run the war file against the content in DFS,
the latency will annoy.  Better to copy the content needed to service
the war application to local disk.  Do the following:
<pre>
  %  ${HADOOP_HOME}/bin/hadoop dfs -get outputs .
</pre>
Set your webapp to point at the local directory as you did for 
standalone mode and restart.
</p>

<h2><a name="next">Where to go next</a></h2>
<p>See <i>Fully-distributed Operation</i> in the hadoop documentation for how
to set Hadoop running on a cluster of more than a single host. 
Also, visit the tutorials and wiki pages up on the nutch and hadoop sites.
In particular a skim of the 
<a href="http://wiki.apache.org/nutch/NutchHadoopTutorial">NutchHadoopTutorial</a>
should prove fruitful.  While nutch-centric -- in particular the way it
pivots on the ${NUTCH_HOME}/bin/nutch command and though the discussion of 
crawling does not apply -- the same general concepts are at the core of
NutchWAX operation.  It also presents at a level of detail beyond that
given here in this overview.
</p>

<h2><a name="configuration">NutchWAX Configuration</a></h2>
<p>To configure your Hadoop install or to override the default
configuration built into the NutchWAX jar or war, add your settings to the
hadoop file <code>hadoop-site.xml</code>.  If your configuration is an index-time
setting, add it to the file at <code>$HADOOP_CONF_DIR/hadoop-site.xml</code>
on all slaves (See <code>${NUTCHWAX_HOME}/bin</code> for a primitive script
to disperse the content of the <code>${HADOOP_CONF_DIR}</code> directory
across the cluster: amend to suit your environment).  If its a search time
configuration, edit the war file
and add your configuration to <code>WEB-INF/classes/hadoop-site.xml</code>.
The content of this file, <code>hadoop-site.xml</code>, file will always win
out over settings in any other file.  Also, be aware of
<code>${HADOOP_CONF_DIR}/hadoop-env.sh</code>.  The contents of this file are
sourced before any hadoop command is run.  It has defines for the location of
the logging directory, <code>${HADOOP_LOGS_DIR}</code>, 
<code>${JAVA_HOME}</code>, etc.</p>

<p>As to what the configuration options are, they are myriad.
There is configuration applicable to nutch, nutchwax and to hadoop.
For the hadoop set, see <a href="http://svn.apache.org/viewvc/lucene/hadoop/trunk/conf/hadoop-default.xml?content-type=text%2Fplain&amp;view=co">hadoop-default.xml</a>, for the nutch set, see <a href= "http://svn.apache.org/viewvc/lucene/nutch/trunk/conf/nutch-default.xml?content-type=text%2Fplain&amp;view=co">nutch-default.xml</a>,
and for nutchwax, see
<a href="https://archive-access.svn.sourceforge.net/svnroot/archive-access/trunk/archive-access/projects/nutch/README.txt">wax-default.xml</a>.
Regards nutchwax, this template file includes a short list of the more
important properties: 
<a href="https://archive-access.svn.sourceforge.net/svnroot/archive-access/trunk/archive-access/projects/nutch/conf/hadoop-site.xml.template">hadoop-site.xml.template</a>.
Out of the box, the NutchWAX configuration should work indexing.  The hadoop
platform will need to be configured with the mapreduce and HDFS head nodes,
the number of children per slave, the number of mappers and reducers to run,
etc.  For search time, the NutchWAX war file will need to know where to find
the created index and how to write the search result URLs (See above under
<a href="#searching">Searching</a>).
</p>

<h2><a name="src">Building NutchWAX from source</a></h2>
<h3>Requirements</h3>
<p>To build from src, you will need to satisfy the above
<a href="#requirements">runtime requirements</a> plus the following.</p>
<ol>
<li><b>Ant</b>: Tested working with version 1.6.2.
 </li>
<li><b>Maven</b>: If you want to build distributions and the website, you'll
need Maven 1.0.2.
 </li>
 <li>For NutchWAX 0.10.0 BRANCH, 
<a href="http://lucene.apache.org/nutch/version_control.html">Nutch Revision: 492357
</a></li> (See the NutchWAX README for details).
</ol>
<p>Checkout NutchWAX [See <a href="http://sourceforge.net/svn/?group_id=118427">Source Repository</a> for how].
</p>
<p>Make a symbolic link under NutchWAX to your Nutch checkout:
<pre>
   % ln -s ${NUTCH_HOME} ${NUTCHWAX_HOME}/nutch
orojects/nutch/project.xml.r1445:       <connection>scm:cvs:pserver:anonymous@archive-access.cvs.sourceforge.net:/cvsroot/archive-access:archive-access/projects/nutch</connection>
</pre>
</p>
<p>Build Nutch:
orojects/nutch/project.xml.mine:            http://sourceforge.net/mailarchive/forum.php?forum=archive-access-cvs
<pre>
   % cd ${NUTCH_HOME}
   % ant jar war
</pre>
</p>
<p>To build NutchWAX, do the same:
<pre>
   % cd ${NUTCHWAX_HOME}
   % ant jar war
   % cd ${NUTCHWAX_HOME}
</pre>
</p>
<p>To build the NutchWAX site or distribution, run maven:
<pre>
   % ${MAVEN_HOME}/bin/maven dist site:generate
</pre>
</p>
<h3>Eclipse</h3>
<p>See <a href="http://archive-access.sourceforge.net/projects/nutch/eclipse.html">Eclipse</a> for notes on how to setup your eclipse environment.</p>
<hr />
<small>$Id$</small>

</body>
</html>
