<?xml version="1.0" encoding="ISO-8859-1"?>

<project name="nutchwax">

  <properties>
    <title>NutchWAX Operation</title>
    <author email="stack at archive dot org">Michael Stack</author>
    <revision>$Id$</revision>
  </properties>
  <body>
  <section name="Overview">
  <p>Below we detail indexing and setting up search 
  of a Web Archive Collection (WAC) of ARCS using
  NutchWAX 0.4.2 or earlier (pre-mapreduce-based NutchWAX).
  Our setup is intentionally simple.  This makes it
  easier to explain.  Assumption that only one server is indexing and that
  only one searcher will be running against a single index: In other words,
  the below is targeted at indexing small collections of ARCs only.
  Parallelizing indexing is not be discussed -- though it should be plain
  how it can be done -- nor will we be discussing Nutch's distributed search
  capability, NDFS nor MapReduce, in the below.</p>

<p>The process of setting up a search of ARCs goes roughly as follows. First we
need to rewrite ARCs into Nutch segments. As the ARCs are rewritten, they're
parsed for their text content, and ARC metadata -- such as name of arcfile
and arcfile offset -- is added as Nutch metadata. When done, Nutch segments
hold a text-only parse of orginal ARC record (Optionally turn on/off parsers
for the various document types). Once all is in segments, we then run a set of
Nutch tools against the segments with the main one being the
Nutch indexing tool.  When thats done, we start up a webapp that can run
queries against the created indices.
</p>

<p>Currently, this tool only indexes documents that return a status code of
200, and only text/* and application/pdf text is extracted.  All other types
have metadata only -- file name, type, arcoffset, arcname, etc., added
to the index.</p>
</section>

<section name="Before you start...">
<p>Before you start, make sure you've satisfied
<a href="requirements.html">requirements</a> and that you've followed
the binary NutchWAX <a href="gettingstarted.html">Getting Started</a>
instructions.</p>

<p>The instructions below use the following defines:
  <ul>
<li>Let the directory
that tomcat resides in be ever after known as ${TOMCAT}.
</li>
<li>Let the directory that contains the NutchWAX install be ${NUTCHWAX}.
</li>
<li>You'll need a place to manufacture and store Nutch segments and indices.
Lets call this ${DATADIR}.</li>
</ul></p>
</section>

<section name="Full Indexing">
<p>
Below we describe how to go about a full indexing. Any of the steps below can
be
restarted on fail (e.g. if you run out of disk indexing overnight).
</p>

<p>The script ${NUTCHWAX}/bin/indexarcs.sh is a shell script that runs
all steps described below.  You might like to read this script in parallel to
the below text (Its at least more succinct). If it will work for your
environment, run it instead of all the
below by hand. Here is the usage for the script indexarcs.sh:
<pre>
    [debord 475] archive-access-nutch > ./bin/indexarcs.sh -h
    Usage: ./bin/indexarcs.sh -h
    Usage: ./bin/indexarcs.sh  [-n] [-t] [-e] -s ARCSDIR -d DATADIR
    Options:
     -h  Prints out this message.
     -s  Directory to 'find' ARCs to index in. Required. This script does
         a find for all files that end in '.arc.gz' under this directory.
     -d  Directory to build the index in. Required. Note, this directory
         is wiped clean every time this script is run.
     -n  Suppress deduplicate step: i.e. Lets duplicates appear in search
         results. Optional.
     -t  Run through steps but don't do anything. Noop used testing flow.
         (Does not turn-off cmdline checking). Optional.
     -e  Expert mode. Pass comma-separated list of internal functions.
         Runs each in order then exits. Functions include: clean,
         setup, segment, links, indexSegments, dedup and merge (Always
         runs 'check'). Optional.
     -f  Pattern to pass find looking for ARCs to segment.  Optional. If
         passed, will be used in place of default '*.arc.gz'
     -a  How many arcs to do per segment. Default is 100.
    This runs through all steps nutch indexing ARCs so their content is
    searchable by nutch. This script is for use against small
    collections only. For big collections, each of the processing steps
    executed will be more involved and will need to be run individually.
    See the README.txt -- https://archive-access.svn.sourceforge.net/svnroot/archive-access/trunk/archive-access/projects/nutch/README.txt -- for more on each individual step. Note that
    this script must be run from inside the archive-access/projects/nutch
    directory: i.e. the ${ARCHIVE_ACCESS} directory as described in the
    README.txt cited above. Running this script, capture all STDERR and
    STDOUT emissions to a log file for later perusal.  You may also want
    to run this script w/ a 'nohup' prefix because even small collections
    can take hours to finish.
</pre>
</p>
<p><pre>
1. If not local, NFS mount disks with ARCs to index.
2. In ${NUTCHWAX}/bin is a script named arcs2segs.sh (which uses
arc2seg.sh from same directory). This script takes two arguments: A 'queue'
directory and a 'segments' directory. Both must exist before segmenting begins.
The 'queue' directory holds symbolic links to all ARCs to index. To make such a
queue directory for say a mounted crawldata00 directory of ARCs, do:
    $ mkdir -p ${DATADIR}/queue
    $ cd ${DATADIR}/queue/
    $ find /mnt/crawldata00*/ -name "E04*arc.gz" -exec ln -s {} \;
3. The segments directory is where the nutch segments produced from the parse
of ARCs get written.
    $ mkdir ${DATADIR}/segments
4. Default behavior for arcs2segs.sh is to index 100 ARCs per nutch index. Each
batch of ARCs will be segmented into a subdirectory of the segments directory.
The name of the subdir will be the date segmenting began on the particular ARC
batch. When arcs2segs.sh has completed segmenting a particular ARC, it removes
the ARC symbolic link from the queue directory and moves it under the segment
batch subdirectory into a directory named 'arcs'. This way you can monitor the
segmenting progress -- which ARCs have finished being segmenting.
5. Now to startup segmenting of all contained in the directory named
'${DATADIR}/queue', do the following:
    $ cd ${NUTCHWAX}
    $ ${NUTCHWAX}/bin/arcs2segs.sh \
        ${DATADIR}/queue/ ${DATADIR}/segments
Catch all output to a log and background the process because it will take
a while to complete. You might also prefix the segmenting with 'nohup' in case
your terminal goes down mid-segmenting.  E.g:
    $ nohup ${NUTCHWAX}/bin/arcs2segs.sh \
        ${DATADIR}/queue/ ${DATADIR}/segments &amp;> /tmp/log.txt &lt; /dev/null
Segmenting needs to be run from inside the ${NUTCHWAX} directory because
at least the pdf plugin expects to find the 'bin/parse-pdf.sh' script relative
to startup location (TODO: THis may no longer be necessary.  Verify).  If you
mess up or forgot something, or segmenting failed
because of out-of-disk-space midway, after addressing the cause of failure, you
can just pick up segmenting where it left off or blow away the segments dirs,
redo the symbolic links in the queue directory and start over.
8. After segmenting has completed, next setup a nutch db in which we'll
calculate anchor text to add to pages to index. First we create the db, then we
tell the db about our just-made segments. Finally we add calculated anchor text
info back to the segments from the db.
    $ ${NUTCHWAX}/bin/nutch admin ${DATADIR}/db -create
    $ ${NUTCHWAX}/bin/nutch updatedb ${DATADIR}/db ${DATADIR}/segments/*
    $ ${NUTCH}/bin/nutch updatesegs ${DATADIR}/db ${DATADIR}/segments \
        ${DATADIR}/tmp 
9. Next, index each segment.
    $ for s in ${DATADIR}/segments/*; do ${NUTCHWAX}/bin/nutch index $s; done
10. After indexing completes, optionally run the dedup step to remove
duplicates. Removing duplicates will make it so you don't have multiple
instances of the one document returned in results. This is usually what you
want -- not having 40 copies of http://www.archive.org, one for each day 
the crawler ran, in your search results -- but sometimes you will want to
see all versions and in this case you'd skip this step. Otherwise:
    $ ${NUTCHWAX}/bin/nutch dedup -workingdir ${DATADIR}/tmp \
        ${DATADIR}/segments
11. After deduping, merge all of the little indexes that are sitting in
the segments by doing the below (One big index is generally better than many
little ones). Run:
    $ ${NUTCHWAX}/bin/nutch merge ${DATADIR}/index ${DATADIR}/segments/*
This command takes time to run and it runs silently.  Monitor the index
size to see if its still in operation.
</pre>
</p>
</section>
<section name="Search setup">
<p>This section describes setup of the search webapp to go against a
collection. The search webapp reads the configuration that is built into it
at build time and it also leverages the tomcat start directory to find
indexes and segments (That the conf. is built into the webapp is a bit of
a problem if you only have the binary version of nutchwax because you've no
means of rebuilding the war file if you want to change the configuration.
To be addressed).
</p>
<p><pre>
1. The nutchwax webapp is in ${NUTCHWAX}/webapps.
2. Copy the warfile under tomcat.
    $ ${TOMCAT}/bin/shutdown.sh
    $ cp $NUTCHWAX/nutchwax.war ${TOMCAT}/webapps/
3. Now, where you start tomcat is critical to nutch finding its search-time
resources. You need to start tomcat from the ${DATADIR}.
    $ cd ${DATADIR}
    $ ${TOMCAT}/bin/startup.sh
(OR, you can edit the nutch-conf.xml and hardcode the location of the
segments directory using the 'search.dir' directive. You'll need to rebuild
the war file with the changed nutch-site.xml).
4. Browse to '/nutchwax' context on your tomcat server. If links returned in
search results are incorrect, edit the nutch-site.xml, rebuild the war file,
reput it into place -- be sure to remove the current war and unbundled war
file from  under tomcat before copying over the new one.  Tomcat will usually
notice the redeployment but a restart is cleanest.
</pre>
</p>
</section>
<section name="Notes">
<p>
You can search against particular fields as you would in google. Here are the
interesting ones (The first few are built into nutch, the latter few have been
added for arc searching). Note some of below are non-scoring, filtering query
clauses. They do not affect ranking. They must be accompanied by a scoring
clause. This is akin to the following at Google: 
http://www.google.com/search?q=filetype%3Apdf:
<pre>
+ url: URL in nutch is tokenized so searches on 'duboce' will return
'www.duboce.net', 'duboce.net', and 'www1.duboce.net' entries.
+ site: Use site keyword to exactly match a particular host.  To search for
the pope on the vatican site only, type in a query 'site:www.vatican.com pope'.
+ host: Use host to query the host field only (Host is tokenized -- will break
query term on '.').
+ collection: To search for 'manchester' in the 'pope' collection, type into the
query box: 'collection:pope manchester' ('collection' is not stored so you
don't see the collection name in search results).
+ date: Takes IA 14 digit timestamps: E.g. 'date:20050505101033 cats pyjamas'.
Ranges can be specified with a '-' delimiter. E.g. To find all pages in range
20050501 to 20050601, type a query of 'date:20050501-20050601 cats pyjamas'
(You must supply a term to search in addition to date because date is a
non-scoring field -- better performance). If you are not specifying ranges
searching on a single date, you must supply a 14 digit IA-style timestamp.
+ arcname: Search on pope content within a particular arc file by doing 
'arcname:iah-20050420054315-00000-box pope'.
+ dedupField: Field to deduplicate on.  Default is 'site'.  Use 'url' to
see only one version of an url for an index with many instances.
+ hitsPerDup: How many matches on 'dedupField' to show in search results. 
Default is '2'. Set to '1' if you would see one instance of an url in results.
Set to '0' to see all.
+ sort: Field to sort results on. No default.
+ reverse: Default is false.  Set to true if you'd see sort in reverse.
+ exacturl: Requires query-exacturl and index-exacturl.  This one is odd in
that it requires that the query be preprocessed before its given to nutch.
If the 'exacturl' subclause is present, then need to encode the clause value
in the same manner in which the exacturl field was encoded.  This is done
so because URL values such as '&amp;', '?', and '=' are hard to get past query
parsers and Nutch Analysis.
</pre>
</p>
</section>
</body>
</project>
